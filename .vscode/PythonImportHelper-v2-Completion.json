[
    {
        "label": "scrapy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scrapy",
        "description": "scrapy",
        "detail": "scrapy",
        "documentation": {}
    },
    {
        "label": "signals",
        "importPath": "scrapy",
        "description": "scrapy",
        "isExtraImport": true,
        "detail": "scrapy",
        "documentation": {}
    },
    {
        "label": "signals",
        "importPath": "scrapy",
        "description": "scrapy",
        "isExtraImport": true,
        "detail": "scrapy",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "NotConfigured",
        "importPath": "scrapy.exceptions",
        "description": "scrapy.exceptions",
        "isExtraImport": true,
        "detail": "scrapy.exceptions",
        "documentation": {}
    },
    {
        "label": "IgnoreRequest",
        "importPath": "scrapy.exceptions",
        "description": "scrapy.exceptions",
        "isExtraImport": true,
        "detail": "scrapy.exceptions",
        "documentation": {}
    },
    {
        "label": "NotConfigured",
        "importPath": "scrapy.exceptions",
        "description": "scrapy.exceptions",
        "isExtraImport": true,
        "detail": "scrapy.exceptions",
        "documentation": {}
    },
    {
        "label": "get_project_settings",
        "importPath": "scrapy.utils.project",
        "description": "scrapy.utils.project",
        "isExtraImport": true,
        "detail": "scrapy.utils.project",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "MongoClient",
        "importPath": "pymongo",
        "description": "pymongo",
        "isExtraImport": true,
        "detail": "pymongo",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "is_item",
        "importPath": "itemadapter",
        "description": "itemadapter",
        "isExtraImport": true,
        "detail": "itemadapter",
        "documentation": {}
    },
    {
        "label": "ItemAdapter",
        "importPath": "itemadapter",
        "description": "itemadapter",
        "isExtraImport": true,
        "detail": "itemadapter",
        "documentation": {}
    },
    {
        "label": "ItemAdapter",
        "importPath": "itemadapter",
        "description": "itemadapter",
        "isExtraImport": true,
        "detail": "itemadapter",
        "documentation": {}
    },
    {
        "label": "HttpProxyMiddleware",
        "importPath": "scrapy.downloadermiddlewares.httpproxy",
        "description": "scrapy.downloadermiddlewares.httpproxy",
        "isExtraImport": true,
        "detail": "scrapy.downloadermiddlewares.httpproxy",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "format_error_msg",
        "importPath": "helpers.error_log_helper",
        "description": "helpers.error_log_helper",
        "isExtraImport": true,
        "detail": "helpers.error_log_helper",
        "documentation": {}
    },
    {
        "label": "init_env",
        "importPath": "helpers.env_config",
        "description": "helpers.env_config",
        "isExtraImport": true,
        "detail": "helpers.env_config",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "APScheduler",
        "importPath": "flask_apscheduler",
        "description": "flask_apscheduler",
        "isExtraImport": true,
        "detail": "flask_apscheduler",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "env_config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "env_config",
        "description": "env_config",
        "detail": "env_config",
        "documentation": {}
    },
    {
        "label": "CrawlSpider",
        "kind": 6,
        "importPath": "crawler.demo.spiders.crawl",
        "description": "crawler.demo.spiders.crawl",
        "peekOfCode": "class CrawlSpider(scrapy.Spider):\n    name = 'crawl_spider'\n    def start_requests(self):\n        for i in range(30):\n            logging.info(str(i))\n            yield scrapy.Request('https://httpbin.org/ip', dont_filter=True)\n    def closed(self, reason):\n        logging.info(\"fuck===============\")\n        logging.info(reason)\n    def parse(self, response):",
        "detail": "crawler.demo.spiders.crawl",
        "documentation": {}
    },
    {
        "label": "ProxySpiders",
        "kind": 6,
        "importPath": "crawler.demo.spiders.proxy",
        "description": "crawler.demo.spiders.proxy",
        "peekOfCode": "class ProxySpiders(scrapy.Spider):\n    name = 'proxy'\n    allowed_domains = ['www.free-proxy-list.net']\n    start_urls = ['https://free-proxy-list.net/']\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'demo.pipelines.JsonWriterPipeline': 300,\n        }\n    }\n    data = []",
        "detail": "crawler.demo.spiders.proxy",
        "documentation": {}
    },
    {
        "label": "ShopSpider",
        "kind": 6,
        "importPath": "crawler.demo.spiders.shop",
        "description": "crawler.demo.spiders.shop",
        "peekOfCode": "class ShopSpider(scrapy.Spider):\n    name = 'shop'\n    allowed_domains = ['www.costco.com.tw']\n    start_urls = ['https://www.costco.com.tw/Health-Beauty/Vitamins-Herbals-Dietary-Supplements/c/701']\n    data=[]\n    def closed(self, reason):\n        logging.info('fucking closed')\n        logging.info(self.data)\n    def parse(self, response):\n        root =BeautifulSoup(response.body,'lxml')",
        "detail": "crawler.demo.spiders.shop",
        "documentation": {}
    },
    {
        "label": "SpiderOpenCloseLogging",
        "kind": 6,
        "importPath": "crawler.demo.extensions",
        "description": "crawler.demo.extensions",
        "peekOfCode": "class SpiderOpenCloseLogging:\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool('MYEXT_ENABLED'):\n            raise NotConfigured",
        "detail": "crawler.demo.extensions",
        "documentation": {}
    },
    {
        "label": "settings",
        "kind": 5,
        "importPath": "crawler.demo.extensions",
        "description": "crawler.demo.extensions",
        "peekOfCode": "settings = get_project_settings()\nlogger = logging.getLogger(__name__)\nclass SpiderOpenCloseLogging:\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise",
        "detail": "crawler.demo.extensions",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "crawler.demo.extensions",
        "description": "crawler.demo.extensions",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SpiderOpenCloseLogging:\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool('MYEXT_ENABLED'):",
        "detail": "crawler.demo.extensions",
        "documentation": {}
    },
    {
        "label": "MyprojectItem",
        "kind": 6,
        "importPath": "crawler.demo.items",
        "description": "crawler.demo.items",
        "peekOfCode": "class MyprojectItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass",
        "detail": "crawler.demo.items",
        "documentation": {}
    },
    {
        "label": "RandomProxyMiddleware",
        "kind": 6,
        "importPath": "crawler.demo.middlewares",
        "description": "crawler.demo.middlewares",
        "peekOfCode": "class RandomProxyMiddleware(HttpProxyMiddleware):\n    def __init__(self, auth_encoding=\"latin-1\", proxy_list_file=None):\n        if not proxy_list_file:\n            raise NotConfigured\n        self.auth_encoding = auth_encoding\n        self.proxies = defaultdict(list)\n        with open(proxy_list_file) as f:\n            proxy_list = json.load(f)\n            for proxy in proxy_list:\n                scheme = proxy[\"scheme\"]",
        "detail": "crawler.demo.middlewares",
        "documentation": {}
    },
    {
        "label": "MyprojectSpiderMiddleware",
        "kind": 6,
        "importPath": "crawler.demo.middlewares",
        "description": "crawler.demo.middlewares",
        "peekOfCode": "class MyprojectSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s",
        "detail": "crawler.demo.middlewares",
        "documentation": {}
    },
    {
        "label": "MyprojectDownloaderMiddleware",
        "kind": 6,
        "importPath": "crawler.demo.middlewares",
        "description": "crawler.demo.middlewares",
        "peekOfCode": "class MyprojectDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s",
        "detail": "crawler.demo.middlewares",
        "documentation": {}
    },
    {
        "label": "JsonWriterPipeline",
        "kind": 6,
        "importPath": "crawler.demo.pipelines",
        "description": "crawler.demo.pipelines",
        "peekOfCode": "class JsonWriterPipeline(object):\n    line=[]\n    def open_spider(self, spider):\n        self.file = open('proxy.json', 'w')\n        # Your scraped items will be saved in the file 'scraped_items.json'.\n        # You can change the filename to whatever you want.\n        # self.file.write(\"[\")\n    def close_spider(self, spider):\n        # self.file.write(self.line[:-2])\n        # self.file.write(\"]\")",
        "detail": "crawler.demo.pipelines",
        "documentation": {}
    },
    {
        "label": "BOT_NAME",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "BOT_NAME = 'demo'\nSPIDER_MODULES = ['demo.spiders']\nNEWSPIDER_MODULE = 'demo.spiders'\nMONGODB_URL=os.getenv('MONGODB_URL')\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'myproject (+http://www.yourdomain.com)'\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "SPIDER_MODULES",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "SPIDER_MODULES = ['demo.spiders']\nNEWSPIDER_MODULE = 'demo.spiders'\nMONGODB_URL=os.getenv('MONGODB_URL')\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'myproject (+http://www.yourdomain.com)'\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n# Configure a delay for requests for the same website (default: 0)",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "NEWSPIDER_MODULE",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "NEWSPIDER_MODULE = 'demo.spiders'\nMONGODB_URL=os.getenv('MONGODB_URL')\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'myproject (+http://www.yourdomain.com)'\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#USER_AGENT",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#USER_AGENT = 'myproject (+http://www.yourdomain.com)'\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "ROBOTSTXT_OBEY",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "ROBOTSTXT_OBEY = False\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#CONCURRENT_REQUESTS",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#CONCURRENT_REQUESTS = 32\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#DOWNLOAD_DELAY",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#CONCURRENT_REQUESTS_PER_DOMAIN",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#CONCURRENT_REQUESTS_PER_IP",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#CONCURRENT_REQUESTS_PER_IP = 16\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#COOKIES_ENABLED",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#COOKIES_ENABLED = False\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#TELNETCONSOLE_ENABLED",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#TELNETCONSOLE_ENABLED = False\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'myproject.middlewares.MyprojectSpiderMiddleware': 543,",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#DEFAULT_REQUEST_HEADERS",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'myproject.middlewares.MyprojectSpiderMiddleware': 543,\n#}\n# Enable or disable downloader middlewares",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#SPIDER_MIDDLEWARES",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#SPIDER_MIDDLEWARES = {\n#    'myproject.middlewares.MyprojectSpiderMiddleware': 543,\n#}\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n# DOWNLOADER_MIDDLEWARES = {\n# #    'myproject.middlewares.MyprojectDownloaderMiddleware': 543,\n# }\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "EXTENSIONS",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "EXTENSIONS = {\n    'demo.extensions.SpiderOpenCloseLogging':666\n   #'scrapy.extensions.telnet.TelnetConsole': None,\n}\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n# ITEM_PIPELINES = {\n# }\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#AUTOTHROTTLE_ENABLED",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#AUTOTHROTTLE_START_DELAY",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#AUTOTHROTTLE_MAX_DELAY",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#AUTOTHROTTLE_TARGET_CONCURRENCY",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#AUTOTHROTTLE_DEBUG",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#AUTOTHROTTLE_DEBUG = False\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n# DOWNLOADER_MIDDLEWARES = {\n#    'myproject.middlewares.RandomProxyMiddleware':745",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#HTTPCACHE_ENABLED",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n# DOWNLOADER_MIDDLEWARES = {\n#    'myproject.middlewares.RandomProxyMiddleware':745\n# }\nPROXY_LIST_FILE = 'proxy.json'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#HTTPCACHE_EXPIRATION_SECS",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n# DOWNLOADER_MIDDLEWARES = {\n#    'myproject.middlewares.RandomProxyMiddleware':745\n# }\nPROXY_LIST_FILE = 'proxy.json'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#HTTPCACHE_DIR",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n# DOWNLOADER_MIDDLEWARES = {\n#    'myproject.middlewares.RandomProxyMiddleware':745\n# }\nPROXY_LIST_FILE = 'proxy.json'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#HTTPCACHE_IGNORE_HTTP_CODES",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n# DOWNLOADER_MIDDLEWARES = {\n#    'myproject.middlewares.RandomProxyMiddleware':745\n# }\nPROXY_LIST_FILE = 'proxy.json'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "#HTTPCACHE_STORAGE",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n# DOWNLOADER_MIDDLEWARES = {\n#    'myproject.middlewares.RandomProxyMiddleware':745\n# }\nPROXY_LIST_FILE = 'proxy.json'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "PROXY_LIST_FILE",
        "kind": 5,
        "importPath": "crawler.demo.settings",
        "description": "crawler.demo.settings",
        "peekOfCode": "PROXY_LIST_FILE = 'proxy.json'",
        "detail": "crawler.demo.settings",
        "documentation": {}
    },
    {
        "label": "init_env",
        "kind": 2,
        "importPath": "crawler.helpers.env_config",
        "description": "crawler.helpers.env_config",
        "peekOfCode": "def init_env():\n    '''\n    初始化環境變數\n    '''\n    if(os.getenv('SYSTEM_ENVIRONMENT')==None):\n        env_path = Path('.') / 'development.env'\n        load_dotenv(dotenv_path=env_path)",
        "detail": "crawler.helpers.env_config",
        "documentation": {}
    },
    {
        "label": "format_error_msg",
        "kind": 2,
        "importPath": "crawler.helpers.error_log_helper",
        "description": "crawler.helpers.error_log_helper",
        "peekOfCode": "def format_error_msg(e):\n    '''\n    格式化錯誤訊息\n    '''\n    error_class = e.__class__.__name__ #取得錯誤類型\n    detail = e.args[0] #取得詳細內容\n    cl, exc, tb = sys.exc_info() #取得Call Stack\n    lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n    fileName = lastCallStack[0] #取得發生的檔案名稱\n    lineNum = lastCallStack[1] #取得發生的行號",
        "detail": "crawler.helpers.error_log_helper",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "schedule.app",
        "description": "schedule.app",
        "peekOfCode": "class Config(object):\n    SCHEDULER_API_ENABLED = True\nscheduler = APScheduler()\napp = Flask(__name__)\napp.config.from_object(Config())\nscheduler.api_enabled = True\nscheduler.init_app(app)\n@scheduler.task('cron', id='do_collect_proxy_ip', hour=9, minute=37, day_of_week='0-6')\ndef do_collect_proxy_ip():\n    '''",
        "detail": "schedule.app",
        "documentation": {}
    },
    {
        "label": "do_collect_proxy_ip",
        "kind": 2,
        "importPath": "schedule.app",
        "description": "schedule.app",
        "peekOfCode": "def do_collect_proxy_ip():\n    '''\n    更新Proxy IP資料\n    '''\n    settings = [\n        ('project', 'default'),\n        ('spider', 'proxy_spider'),\n    ]\n    logging.info('--------------------------'+crawl_url)\n    r = requests.post(f'{crawl_url}/schedule.json', data=settings)",
        "detail": "schedule.app",
        "documentation": {}
    },
    {
        "label": "scheduler",
        "kind": 5,
        "importPath": "schedule.app",
        "description": "schedule.app",
        "peekOfCode": "scheduler = APScheduler()\napp = Flask(__name__)\napp.config.from_object(Config())\nscheduler.api_enabled = True\nscheduler.init_app(app)\n@scheduler.task('cron', id='do_collect_proxy_ip', hour=9, minute=37, day_of_week='0-6')\ndef do_collect_proxy_ip():\n    '''\n    更新Proxy IP資料\n    '''",
        "detail": "schedule.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "schedule.app",
        "description": "schedule.app",
        "peekOfCode": "app = Flask(__name__)\napp.config.from_object(Config())\nscheduler.api_enabled = True\nscheduler.init_app(app)\n@scheduler.task('cron', id='do_collect_proxy_ip', hour=9, minute=37, day_of_week='0-6')\ndef do_collect_proxy_ip():\n    '''\n    更新Proxy IP資料\n    '''\n    settings = [",
        "detail": "schedule.app",
        "documentation": {}
    },
    {
        "label": "scheduler.api_enabled",
        "kind": 5,
        "importPath": "schedule.app",
        "description": "schedule.app",
        "peekOfCode": "scheduler.api_enabled = True\nscheduler.init_app(app)\n@scheduler.task('cron', id='do_collect_proxy_ip', hour=9, minute=37, day_of_week='0-6')\ndef do_collect_proxy_ip():\n    '''\n    更新Proxy IP資料\n    '''\n    settings = [\n        ('project', 'default'),\n        ('spider', 'proxy_spider'),",
        "detail": "schedule.app",
        "documentation": {}
    },
    {
        "label": "init_env",
        "kind": 2,
        "importPath": "schedule.env_config",
        "description": "schedule.env_config",
        "peekOfCode": "def init_env():\n    '''\n    初始化環境變數\n    '''\n    if(os.getenv('SYSTEM_ENVIRONMENT')==None):\n        env_path = Path('.') / 'development.env'\n        load_dotenv(dotenv_path=env_path)",
        "detail": "schedule.env_config",
        "documentation": {}
    }
]